{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "qitpanwp7mkoa5f7dttp",
   "authorId": "1588298242866",
   "authorName": "TTEIXEIRA",
   "authorEmail": "thiago.teixeira@snowflake.com",
   "sessionId": "44389601-1b17-476c-b991-417c0f12282e",
   "lastEditTime": 1757552606584
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d54e712-16fd-4865-934f-665cc5bb64ee",
   "metadata": {
    "name": "instructions",
    "collapsed": false
   },
   "source": "# Instructions\n\n1. In `Notebook Settings > External Access`, turn on `ST_ASSISTANT_EXTERNAL_INTEGRATIONS`\n1. Try running the notebook once to make sure it works\n1. If all good, click on the calendar icon to schedule this notebook to rerun every day. "
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "source": "import json\nimport pandas as pd\nimport re\nimport requests\nimport streamlit as st\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, RecursiveJsonSplitter\nfrom packaging import version\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d14a4679-b28f-48f1-beb4-4bff2c0e6040",
   "metadata": {
    "language": "sql",
    "name": "set_up_role"
   },
   "outputs": [],
   "source": "use role st_assistant_pipeline;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "prepare_docs_pages"
   },
   "source": "def get_docs_pages_df():\n    PAGE_SEP_RE = re.compile(\"^---$\", flags=re.MULTILINE)\n    URL_RE = re.compile(\"^Source: (.*)$\", flags=re.MULTILINE)\n    \n    url = \"https://docs.streamlit.io/llms-full.txt\"\n    full_str = requests.get(url).text\n    page_strs = PAGE_SEP_RE.split(full_str)\n    \n    text_splitter = RecursiveCharacterTextSplitter()\n    page_table_rows = []\n    \n    for page_str in page_strs:\n        url = None\n    \n        for match in URL_RE.finditer(page_str):\n            if match.lastindex == 1:\n                url = match[1]\n                break\n    \n        chunks = text_splitter.split_text(page_str)\n    \n        for chunk in chunks:\n            page_table_rows.append(\n                dict(\n                    PAGE_URL=url,\n                    PAGE_CHUNK=chunk,\n                )\n            )\n    \n    return pd.DataFrame(page_table_rows)\n\ndocs_pages_df = get_docs_pages_df()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ba3211e5-418e-4988-a6b0-d85e56aee2e7",
   "metadata": {
    "language": "python",
    "name": "prepare_docstrings"
   },
   "outputs": [],
   "source": "def update_dict_with_latest_streamlit_version(docstrings_dict):\n    all_versions = []\n\n    for v_str in docstrings_dict.keys():\n        try:\n            v = version.parse(v_str)\n        except version.InvalidVersion:\n            continue\n\n        all_versions.append(v)\n\n    latest_version = max(all_versions)\n    docstrings_dict[\"latest\"] = docstrings_dict[str(latest_version)]\n\n    print(\"Detected latest Streamlit version as \", latest_version)\n\n\ndef get_docstrings_df():\n    json_splitter = RecursiveJsonSplitter()\n    \n    url = \"https://raw.githubusercontent.com/streamlit/docs/refs/heads/main/python/streamlit.json\"\n    full_str = requests.get(url).text\n    docstrings_dict = json.loads(full_str)\n    \n    update_dict_with_latest_streamlit_version(docstrings_dict)\n    \n    docstrings_table_rows = []\n    \n    for st_version, version_docs in docstrings_dict.items():\n        for command_name, command_docstring_obj in version_docs.items():\n            chunks = json_splitter.split_text(command_docstring_obj)\n    \n            for chunk in chunks:\n                docstrings_table_rows.append(\n                    dict(\n                        STREAMLIT_VERSION=st_version,\n                        COMMAND_NAME=command_name,\n                        DOCSTRING_CHUNK=chunk,\n                    )\n                )\n\n    return pd.DataFrame(docstrings_table_rows)\n\ndocstrings_df = get_docstrings_df()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6007c60-eff5-4025-bcdf-e739368b64c9",
   "metadata": {
    "language": "sql",
    "name": "delete_old_data"
   },
   "outputs": [],
   "source": "TRUNCATE TABLE ST_ASSISTANT.PUBLIC.STREAMLIT_DOCSTRINGS_CHUNKS;\nTRUNCATE TABLE ST_ASSISTANT.PUBLIC.STREAMLIT_DOCS_PAGES_CHUNKS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0589b98-d2c2-44e0-bbb8-a1a28d712e6c",
   "metadata": {
    "language": "python",
    "name": "insert_new_data",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.write_pandas(\n    docs_pages_df,\n    database=\"ST_ASSISTANT\",\n    schema=\"PUBLIC\",\n    table_name=\"STREAMLIT_DOCS_PAGES_CHUNKS\",\n)\n\nsession.write_pandas(\n    docstrings_df,\n    database=\"ST_ASSISTANT\",\n    schema=\"PUBLIC\",\n    table_name=\"STREAMLIT_DOCSTRINGS_CHUNKS\",\n)\n\n\"Done!\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "efd87c1a-9482-4b56-9962-d6bcafc7f048",
   "metadata": {
    "language": "python",
    "name": "quick_check"
   },
   "outputs": [],
   "source": "st.write(\"# Quick check\")\n\nfor table in [\"STREAMLIT_DOCS_PAGES_CHUNKS\", \"STREAMLIT_DOCSTRINGS_CHUNKS\"]:\n\n    st.write(f\"## Table `{table}`\")\n    \n    df = session.sql(f\"SELECT COUNT(1) FROM ST_ASSISTANT.PUBLIC.{table}\").to_pandas()\n    st.metric(\"Number of rows\", df.iat[0, 0])\n    \n    st.write(\"#### Data sample\")\n    df = session.sql(f\"SELECT * FROM ST_ASSISTANT.PUBLIC.{table} LIMIT 100\").to_pandas()\n    st.write(df)",
   "execution_count": null
  }
 ]
}
